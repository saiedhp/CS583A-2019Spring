{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Home 3: Build a CNN for image recognition.\n",
    "\n",
    "### Name: [Your-Name?]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. You will do the following:\n",
    "\n",
    "1. Read, complete, and run my code.\n",
    "\n",
    "2. **Make substantial improvements** to maximize the accurcy.\n",
    "    \n",
    "3. Convert the .IPYNB file to .HTML file.\n",
    "\n",
    "    * The HTML file must contain the code and the output after execution.\n",
    "    \n",
    "4. Upload this .HTML file to your Github repo.\n",
    "\n",
    "4. Submit the link to this .HTML file to Canvas.\n",
    "\n",
    "    * Example: https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM3/cnn.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train: (50000, 32, 32, 3)\n",
      "shape of y_train: (50000, 1)\n",
      "shape of x_test: (10000, 32, 32, 3)\n",
      "shape of y_test: (10000, 1)\n",
      "number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import cifar10\n",
    "import numpy\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print('shape of x_train: ' + str(x_train.shape))\n",
    "print('shape of y_train: ' + str(y_train.shape))\n",
    "print('shape of x_test: ' + str(x_test.shape))\n",
    "print('shape of y_test: ' + str(y_test.shape))\n",
    "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. One-hot encode the labels\n",
    "\n",
    "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
    "\n",
    "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
    "\n",
    "2. Apply the function to ```y_train``` and ```y_test```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of y_train_vec: (50000, 10)\n",
      "Shape of y_test_vec: (10000, 10)\n",
      "[6]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "def to_one_hot(y, num_class=10):\n",
    "    one_hot = numpy.zeros((len(y), num_class))\n",
    "    for i in range(len(y)):\n",
    "        one_hot[i, y[i]] = 1\n",
    "    return one_hot\n",
    "\n",
    "y_train_vec = to_one_hot(y_train)\n",
    "y_test_vec = to_one_hot(y_test)\n",
    "\n",
    "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
    "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
    "\n",
    "print(y_train[0])\n",
    "print(y_train_vec[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remark: the outputs should be\n",
    "* Shape of y_train_vec: (50000, 10)\n",
    "* Shape of y_test_vec: (10000, 10)\n",
    "* [6]\n",
    "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Randomly partition the training set to training and validation sets\n",
    "\n",
    "Randomly partition the 50K training samples to 2 sets:\n",
    "* a training set containing 40K samples\n",
    "* a validation set containing 10K samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_tr: (40000, 32, 32, 3)\n",
      "Shape of y_tr: (40000, 10)\n",
      "Shape of x_val: (10000, 32, 32, 3)\n",
      "Shape of y_val: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "rand_indices = numpy.random.permutation(50000)\n",
    "train_indices = rand_indices[0:40000]\n",
    "valid_indices = rand_indices[40000:50000]\n",
    "\n",
    "# train_indices = rand_indices[0:4000]\n",
    "# valid_indices = rand_indices[4000:5000]\n",
    "\n",
    "x_val = x_train[valid_indices, :]\n",
    "y_val = y_train_vec[valid_indices, :]\n",
    "\n",
    "x_tr = x_train[train_indices, :]\n",
    "y_tr = y_train_vec[train_indices, :]\n",
    "\n",
    "print('Shape of x_tr: ' + str(x_tr.shape))\n",
    "print('Shape of y_tr: ' + str(y_tr.shape))\n",
    "print('Shape of x_val: ' + str(x_val.shape))\n",
    "print('Shape of y_val: ' + str(y_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Build a CNN and tune its hyper-parameters\n",
    "\n",
    "1. Build a convolutional neural network model\n",
    "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
    "    * Do NOT use test data for hyper-parameter tuning!!!\n",
    "3. Try to achieve a validation accuracy as high as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remark: \n",
    "\n",
    "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
    "* Add more layers.\n",
    "* Use regularizations, e.g., dropout.\n",
    "* Use batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_215 (Conv2D)          (None, 32, 32, 32)        2432      \n",
      "_________________________________________________________________\n",
      "batch_normalization_55 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_47 (Activation)   (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_216 (Conv2D)          (None, 32, 32, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_87 (MaxPooling (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_217 (Conv2D)          (None, 16, 16, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_218 (Conv2D)          (None, 16, 16, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_88 (MaxPooling (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_219 (Conv2D)          (None, 8, 8, 128)         73856     \n",
      "_________________________________________________________________\n",
      "conv2d_220 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "conv2d_221 (Conv2D)          (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "flatten_37 (Flatten)         (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 16384)             0         \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 128)               2097280   \n",
      "_________________________________________________________________\n",
      "batch_normalization_56 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_48 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 2,682,922\n",
      "Trainable params: 2,682,602\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Add, BatchNormalization, Activation, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(Conv2D(32, (5, 5), padding='same', input_shape=(32, 32, 3)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Dropout(rate=0.5))\n",
    "model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(Conv2D(256, (3, 3), activation='relu', padding='same'))\n",
    "# model.add(MaxPooling2D((2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dense(256, activation='relu'))\n",
    "# model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "model.add(Dense(128))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "learning_rate = 5E-5 # to be tuned!\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer=optimizers.RMSprop(lr=learning_rate),\n",
    "              optimizer=optimizers.Adam(lr=learning_rate),\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/100\n",
      "40000/40000 [==============================] - 129s 3ms/step - loss: 1.3863 - acc: 0.5087 - val_loss: 1.2391 - val_acc: 0.5610\n",
      "Epoch 2/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.1521 - acc: 0.5962 - val_loss: 1.0736 - val_acc: 0.6222\n",
      "Epoch 3/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 1.0078 - acc: 0.6492 - val_loss: 0.9947 - val_acc: 0.6520\n",
      "Epoch 4/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.8972 - acc: 0.6887 - val_loss: 0.9129 - val_acc: 0.6809\n",
      "Epoch 5/100\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.8025 - acc: 0.7249 - val_loss: 0.9161 - val_acc: 0.6793\n",
      "Epoch 6/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.7278 - acc: 0.7517 - val_loss: 0.9540 - val_acc: 0.6709\n",
      "Epoch 7/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.6513 - acc: 0.7774 - val_loss: 0.8428 - val_acc: 0.7053\n",
      "Epoch 8/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.5846 - acc: 0.8029 - val_loss: 0.8924 - val_acc: 0.6933\n",
      "Epoch 9/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.5295 - acc: 0.8221 - val_loss: 0.8298 - val_acc: 0.7179\n",
      "Epoch 10/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4690 - acc: 0.8441 - val_loss: 0.8300 - val_acc: 0.7184\n",
      "Epoch 11/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.4208 - acc: 0.8618 - val_loss: 0.8351 - val_acc: 0.7176\n",
      "Epoch 12/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3763 - acc: 0.8784 - val_loss: 0.8123 - val_acc: 0.7238\n",
      "Epoch 13/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3379 - acc: 0.8911 - val_loss: 0.7945 - val_acc: 0.7313\n",
      "Epoch 14/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.3031 - acc: 0.9026 - val_loss: 0.8045 - val_acc: 0.7388\n",
      "Epoch 15/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2751 - acc: 0.9122 - val_loss: 0.7982 - val_acc: 0.7411\n",
      "Epoch 16/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2448 - acc: 0.9229 - val_loss: 0.8472 - val_acc: 0.7312\n",
      "Epoch 17/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2182 - acc: 0.9318 - val_loss: 0.8268 - val_acc: 0.7363\n",
      "Epoch 18/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.2001 - acc: 0.9372 - val_loss: 0.9013 - val_acc: 0.7215\n",
      "Epoch 19/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1888 - acc: 0.9404 - val_loss: 0.9267 - val_acc: 0.7176\n",
      "Epoch 20/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1727 - acc: 0.9475 - val_loss: 0.8608 - val_acc: 0.7361\n",
      "Epoch 21/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1600 - acc: 0.9494 - val_loss: 0.8580 - val_acc: 0.7415\n",
      "Epoch 22/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1518 - acc: 0.9526 - val_loss: 0.8456 - val_acc: 0.7437\n",
      "Epoch 23/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1388 - acc: 0.9577 - val_loss: 0.8967 - val_acc: 0.7376\n",
      "Epoch 24/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1305 - acc: 0.9598 - val_loss: 0.9309 - val_acc: 0.7339\n",
      "Epoch 25/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1235 - acc: 0.9607 - val_loss: 0.9585 - val_acc: 0.7312\n",
      "Epoch 26/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1140 - acc: 0.9642 - val_loss: 0.8728 - val_acc: 0.7501\n",
      "Epoch 27/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1151 - acc: 0.9639 - val_loss: 0.8947 - val_acc: 0.7468\n",
      "Epoch 28/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1055 - acc: 0.9679 - val_loss: 0.9111 - val_acc: 0.7508\n",
      "Epoch 29/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.1027 - acc: 0.9669 - val_loss: 0.8970 - val_acc: 0.7515\n",
      "Epoch 30/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.0974 - acc: 0.9695 - val_loss: 0.9903 - val_acc: 0.7339\n",
      "Epoch 31/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.0948 - acc: 0.9697 - val_loss: 0.9818 - val_acc: 0.7364\n",
      "Epoch 32/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0931 - acc: 0.9697 - val_loss: 0.9518 - val_acc: 0.7488\n",
      "Epoch 33/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.0885 - acc: 0.9717 - val_loss: 0.9146 - val_acc: 0.7536\n",
      "Epoch 34/100\n",
      "40000/40000 [==============================] - 122s 3ms/step - loss: 0.0815 - acc: 0.9751 - val_loss: 0.9533 - val_acc: 0.7470\n",
      "Epoch 35/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0862 - acc: 0.9722 - val_loss: 0.9738 - val_acc: 0.7409\n",
      "Epoch 36/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0791 - acc: 0.9737 - val_loss: 1.0245 - val_acc: 0.7355\n",
      "Epoch 37/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0797 - acc: 0.9747 - val_loss: 0.9599 - val_acc: 0.7508\n",
      "Epoch 38/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0746 - acc: 0.9762 - val_loss: 0.9808 - val_acc: 0.7458\n",
      "Epoch 39/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0778 - acc: 0.9748 - val_loss: 1.0035 - val_acc: 0.7422\n",
      "Epoch 40/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0705 - acc: 0.9778 - val_loss: 1.0203 - val_acc: 0.7443\n",
      "Epoch 41/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0707 - acc: 0.9782 - val_loss: 1.0112 - val_acc: 0.7433\n",
      "Epoch 42/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0667 - acc: 0.9782 - val_loss: 0.9586 - val_acc: 0.7539\n",
      "Epoch 43/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0678 - acc: 0.9793 - val_loss: 1.1041 - val_acc: 0.7328\n",
      "Epoch 44/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0673 - acc: 0.9779 - val_loss: 1.0385 - val_acc: 0.7423\n",
      "Epoch 45/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0634 - acc: 0.9794 - val_loss: 1.0261 - val_acc: 0.7519\n",
      "Epoch 46/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0656 - acc: 0.9787 - val_loss: 1.0285 - val_acc: 0.7474\n",
      "Epoch 47/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.0627 - acc: 0.9797 - val_loss: 1.0636 - val_acc: 0.7433\n",
      "Epoch 48/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.0632 - acc: 0.9798 - val_loss: 1.0658 - val_acc: 0.7412\n",
      "Epoch 49/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0573 - acc: 0.9818 - val_loss: 1.0697 - val_acc: 0.7386\n",
      "Epoch 50/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0587 - acc: 0.9807 - val_loss: 1.0757 - val_acc: 0.7441\n",
      "Epoch 51/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.0573 - acc: 0.9815 - val_loss: 1.1729 - val_acc: 0.7373\n",
      "Epoch 52/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.0577 - acc: 0.9811 - val_loss: 1.0212 - val_acc: 0.7524\n",
      "Epoch 53/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0563 - acc: 0.9817 - val_loss: 1.0058 - val_acc: 0.7507\n",
      "Epoch 54/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.0574 - acc: 0.9815 - val_loss: 1.0380 - val_acc: 0.7498\n",
      "Epoch 55/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.0518 - acc: 0.9834 - val_loss: 1.0232 - val_acc: 0.7581\n",
      "Epoch 56/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0540 - acc: 0.9824 - val_loss: 1.0380 - val_acc: 0.7530\n",
      "Epoch 57/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.0493 - acc: 0.9836 - val_loss: 1.1169 - val_acc: 0.7378\n",
      "Epoch 58/100\n",
      "40000/40000 [==============================] - 123s 3ms/step - loss: 0.0492 - acc: 0.9843 - val_loss: 1.0812 - val_acc: 0.7506\n",
      "Epoch 59/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.0517 - acc: 0.9830 - val_loss: 0.9872 - val_acc: 0.7631\n",
      "Epoch 60/100\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.0504 - acc: 0.9831 - val_loss: 1.0688 - val_acc: 0.7473\n",
      "Epoch 61/100\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.0486 - acc: 0.9837 - val_loss: 1.0471 - val_acc: 0.7532\n",
      "Epoch 62/100\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.0450 - acc: 0.9859 - val_loss: 1.0086 - val_acc: 0.7550\n",
      "Epoch 63/100\n",
      "40000/40000 [==============================] - 121s 3ms/step - loss: 0.0473 - acc: 0.9842 - val_loss: 1.0920 - val_acc: 0.7435\n",
      "Epoch 64/100\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.0457 - acc: 0.9856 - val_loss: 1.1644 - val_acc: 0.7433\n",
      "Epoch 65/100\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.0508 - acc: 0.9830 - val_loss: 1.0362 - val_acc: 0.7550\n",
      "Epoch 66/100\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.0481 - acc: 0.9848 - val_loss: 1.0775 - val_acc: 0.7543\n",
      "Epoch 67/100\n",
      "40000/40000 [==============================] - 120s 3ms/step - loss: 0.0438 - acc: 0.9863 - val_loss: 1.1501 - val_acc: 0.7476\n",
      "Epoch 68/100\n",
      "40000/40000 [==============================] - 124s 3ms/step - loss: 0.0446 - acc: 0.9851 - val_loss: 1.0702 - val_acc: 0.7569\n",
      "Epoch 69/100\n",
      "40000/40000 [==============================] - 126s 3ms/step - loss: 0.0443 - acc: 0.9860 - val_loss: 1.1469 - val_acc: 0.7489\n",
      "Epoch 70/100\n",
      "40000/40000 [==============================] - 127s 3ms/step - loss: 0.0449 - acc: 0.9855 - val_loss: 1.1033 - val_acc: 0.7505\n",
      "Epoch 71/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.0384 - acc: 0.9877 - val_loss: 1.1000 - val_acc: 0.7518\n",
      "Epoch 72/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0449 - acc: 0.9846 - val_loss: 1.1235 - val_acc: 0.7505\n",
      "Epoch 73/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.0384 - acc: 0.9880 - val_loss: 1.0537 - val_acc: 0.7627\n",
      "Epoch 74/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0409 - acc: 0.9872 - val_loss: 1.0882 - val_acc: 0.7642\n",
      "Epoch 75/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0401 - acc: 0.9867 - val_loss: 1.1240 - val_acc: 0.7523\n",
      "Epoch 76/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0431 - acc: 0.9861 - val_loss: 1.3489 - val_acc: 0.7301\n",
      "Epoch 77/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0390 - acc: 0.9869 - val_loss: 1.1112 - val_acc: 0.7565\n",
      "Epoch 78/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.0433 - acc: 0.9860 - val_loss: 1.0768 - val_acc: 0.7543\n",
      "Epoch 79/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.0403 - acc: 0.9867 - val_loss: 1.1208 - val_acc: 0.7522\n",
      "Epoch 80/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.0398 - acc: 0.9868 - val_loss: 1.1311 - val_acc: 0.7529\n",
      "Epoch 81/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.0369 - acc: 0.9874 - val_loss: 1.2734 - val_acc: 0.7395\n",
      "Epoch 82/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0377 - acc: 0.9881 - val_loss: 1.1279 - val_acc: 0.7647\n",
      "Epoch 83/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0396 - acc: 0.9867 - val_loss: 1.0838 - val_acc: 0.7590\n",
      "Epoch 84/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.0381 - acc: 0.9883 - val_loss: 1.1182 - val_acc: 0.7534\n",
      "Epoch 85/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0373 - acc: 0.9879 - val_loss: 1.1281 - val_acc: 0.7559\n",
      "Epoch 86/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0407 - acc: 0.9869 - val_loss: 1.0521 - val_acc: 0.7674\n",
      "Epoch 87/100\n",
      "40000/40000 [==============================] - 132s 3ms/step - loss: 0.0337 - acc: 0.9890 - val_loss: 1.0884 - val_acc: 0.7671\n",
      "Epoch 88/100\n",
      "40000/40000 [==============================] - 133s 3ms/step - loss: 0.0372 - acc: 0.9874 - val_loss: 1.2363 - val_acc: 0.7524\n",
      "Epoch 89/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.0351 - acc: 0.9884 - val_loss: 1.1033 - val_acc: 0.7529\n",
      "Epoch 90/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.0340 - acc: 0.9886 - val_loss: 1.1438 - val_acc: 0.7581\n",
      "Epoch 91/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.0343 - acc: 0.9889 - val_loss: 1.0679 - val_acc: 0.7510\n",
      "Epoch 92/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.0335 - acc: 0.9894 - val_loss: 1.0943 - val_acc: 0.7557\n",
      "Epoch 93/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.0341 - acc: 0.9888 - val_loss: 1.3861 - val_acc: 0.7192\n",
      "Epoch 94/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.0360 - acc: 0.9878 - val_loss: 1.1119 - val_acc: 0.7575\n",
      "Epoch 95/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.0378 - acc: 0.9874 - val_loss: 1.1051 - val_acc: 0.7634\n",
      "Epoch 96/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.0290 - acc: 0.9908 - val_loss: 1.2376 - val_acc: 0.7404\n",
      "Epoch 97/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.0325 - acc: 0.9896 - val_loss: 1.1351 - val_acc: 0.7577\n",
      "Epoch 98/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.0362 - acc: 0.9876 - val_loss: 1.1635 - val_acc: 0.7504\n",
      "Epoch 99/100\n",
      "40000/40000 [==============================] - 135s 3ms/step - loss: 0.0350 - acc: 0.9884 - val_loss: 1.1978 - val_acc: 0.7582\n",
      "Epoch 100/100\n",
      "40000/40000 [==============================] - 134s 3ms/step - loss: 0.0297 - acc: 0.9905 - val_loss: 1.1806 - val_acc: 0.7625\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_tr, y_tr, batch_size=32, epochs=100, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train (again) and evaluate the model\n",
    "\n",
    "- To this end, you have found the \"best\" hyper-parameters. \n",
    "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
    "- Evaluate your model on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Train the model on the entire training set\n",
    "\n",
    "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Compile your model again (using the same hyper-parameters)>\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<Train your model on the entire training set (50K samples)>\n",
    "<Use (x_train, y_train_vec) instead of (x_tr, y_tr)>\n",
    "<Do NOT use the validation_data option (because now you do not have validation data)>\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Evaluate the model on the test set\n",
    "\n",
    "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_acc = model.evaluate(x_test, y_test_vec)\n",
    "print('loss = ' + str(loss_and_acc[0]))\n",
    "print('accuracy = ' + str(loss_and_acc[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
