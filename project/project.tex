\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,amsmath,amsthm,amsfonts}
\usepackage{latexsym,graphicx}
\usepackage{fullpage,color}
\usepackage{url,hyperref}
\usepackage{natbib}
\usepackage{graphicx,subfigure}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{color}

\numberwithin{equation}{section}

\pagestyle{plain}

\setlength{\oddsidemargin}{0in}
\setlength{\topmargin}{0in}
\setlength{\textwidth}{6.5in}
\setlength{\textheight}{8.5in}

\newtheorem{fact}{Fact}[section]
\newtheorem{question}{Question}[section]
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{assumption}[lemma]{Assumption}
\newtheorem{corollary}[lemma]{Corollary}
\newtheorem{prop}[lemma]{Proposition}
\newtheorem{claim}{Claim}[section]
\newtheorem{remark}{Remark}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{prob}{Problem}[section]
\newtheorem{conjecture}{Conjecture}[section]
\newtheorem{property}{Property}[section]

\def\A{{\bf A}}
\def\a{{\bf a}}
\def\B{{\bf B}}
\def\bb{{\bf b}}
\def\C{{\bf C}}
\def\c{{\bf c}}
\def\D{{\bf D}}
\def\d{{\bf d}}
\def\E{{\bf E}}
\def\e{{\bf e}}
\def\F{{\bf F}}
\def\f{{\bf f}}
\def\g{{\bf g}}
\def\h{{\bf h}}
\def\G{{\bf G}}
\def\H{{\bf H}}
\def\I{{\bf I}}
\def\K{{\bf K}}
\def\k{{\bf k}}
\def\LL{{\bf L}}
\def\M{{\bf M}}
\def\m{{\bf m}}
\def\N{{\bf N}}
\def\n{{\bf n}}
\def\PP{{\bf P}}
\def\Q{{\bf Q}}
\def\q{{\bf q}}
\def\R{{\bf R}}
\def\rr{{\bf r}}
\def\S{{\bf S}}
\def\s{{\bf s}}
\def\T{{\bf T}}
\def\tt{{\bf t}}
\def\U{{\bf U}}
\def\u{{\bf u}}
\def\V{{\bf V}}
\def\v{{\bf v}}
\def\W{{\bf W}}
\def\w{{\bf w}}
\def\X{{\bf X}}
\def\x{{\bf x}}
\def\Y{{\bf Y}}
\def\y{{\bf y}}
\def\Z{{\bf Z}}
\def\z{{\bf z}}
\def\0{{\bf 0}}
\def\1{{\bf 1}}



\def\AM{{\mathcal A}}
\def\CM{{\mathcal C}}
\def\DM{{\mathcal D}}
\def\EM{{\mathcal E}}
\def\GM{{\mathcal G}}
\def\FM{{\mathcal F}}
\def\IM{{\mathcal I}}
\def\JM{{\mathcal J}}
\def\KM{{\mathcal K}}
\def\LM{{\mathcal L}}
\def\NM{{\mathcal N}}
\def\OM{{\mathcal O}}
\def\PM{{\mathcal P}}
\def\SM{{\mathcal S}}
\def\TM{{\mathcal T}}
\def\UM{{\mathcal U}}
\def\VM{{\mathcal V}}
\def\WM{{\mathcal W}}
\def\XM{{\mathcal X}}
\def\YM{{\mathcal Y}}
\def\RB{{\mathbb R}}
\def\RBmn{{\RB^{m\times n}}}
\def\EB{{\mathbb E}}
\def\PB{{\mathbb P}}


\def\alp{\mbox{\boldmath$\alpha$\unboldmath}}
\def\bet{\mbox{\boldmath$\beta$\unboldmath}}
\def\epsi{\mbox{\boldmath$\epsilon$\unboldmath}}
\def\etab{\mbox{\boldmath$\eta$\unboldmath}}
\def\ph{\mbox{\boldmath$\phi$\unboldmath}}
\def\pii{\mbox{\boldmath$\pi$\unboldmath}}
\def\Ph{\mbox{\boldmath$\Phi$\unboldmath}}
\def\Ps{\mbox{\boldmath$\Psi$\unboldmath}}
\def\ps{\mbox{\boldmath$\psi$\unboldmath}}
\def\tha{\mbox{\boldmath$\theta$\unboldmath}}
\def\Tha{\mbox{\boldmath$\Theta$\unboldmath}}
\def\muu{\mbox{\boldmath$\mu$\unboldmath}}
\def\Si{\mbox{\boldmath$\Sigma$\unboldmath}}
\def\si{\mbox{\boldmath$\sigma$\unboldmath}}
\def\Gam{\mbox{\boldmath$\Gamma$\unboldmath}}
\def\Lam{\mbox{\boldmath$\Lambda$\unboldmath}}
\def\De{\mbox{\boldmath$\Delta$\unboldmath}}
\def\Ome{\mbox{\boldmath$\Omega$\unboldmath}}
\def\Pii{\mbox{\boldmath$\Pi$\unboldmath}}
\def\varepsi{\mbox{\boldmath$\varepsilon$\unboldmath}}

\def\argmax{\mathop{\rm argmax}}
\def\argmin{\mathop{\rm argmin}}
\def\bias{\mathsf{bias}}
\def\var{\mathsf{var}}
\def\sgn{\mathsf{sgn}}
\def\tr{\mathsf{tr}}
\def\rk{\mathrm{rank}}
\def\poly{\mathrm{poly}}
\def\diag{\mathsf{diag}}
\def\st{\mathsf{s.t.}}

\newcommand{\red}[1]{{\color{red}#1}}



\lstset{ %
extendedchars=false,            % Shutdown no-ASCII compatible
language=Python,                % choose the language of the code
xleftmargin=1em,
xrightmargin=1em,
basicstyle=\footnotesize,    % the size of the fonts that are used for the code
tabsize=3,                            % sets default tabsize to 3 spaces
numbers=left,                   % where to put the line-numbers
numberstyle=\tiny,              % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it's 1 each line
                                % will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code   %
keywordstyle=\color[rgb]{0,0,1},                % keywords
commentstyle=\color[rgb]{0.133,0.545,0.133},    % comments
stringstyle=\color[rgb]{0.627,0.126,0.941},      % strings
backgroundcolor=\color{white}, % choose the background color. You must add \usepackage{color}
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                 % adds a frame around the code
%captionpos=b,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
%title=\lstname,                 % show the filename of files included with \lstinputlisting;
%                                % also try caption instead of title
mathescape=true,escapechar=?    % escape to latex with ?..?
escapeinside={\%*}{*)},         % if you want to add a comment within your code
%columns=fixed,                  % nice spacing
%morestring=[m]',                % strings
%morekeywords={%,...},%          % if you want to add more keywords to the set
%    break,case,catch,continue,elseif,else,end,for,function,global,%
%    if,otherwise,persistent,return,switch,try,while,...},%
}


\begin{document}


\title{CS583A: Course Project}

\author{Saeid Hosseinipoor}

\date{May 17, 2019 }

\maketitle




\section{Summary}

I participate an inactive with late submission competition of Humpback Whale Identification based on photos. 
The final model I choose is Open Face, a deep convolutional neural network architecture, which is a Siamese network of a variant of Inception model takes $192\times 192$ images and outputs feature vector of the image. To predict the class of test image we find nearest neighbors of the feature vector in the same space of training set outputs.
I implement the convolutional neural network using Keras and run the code on a PC with one Intel i7 CPU and 64 GB memory.
Performance is evaluated on the top-5 classification accuracy metrics.
In the public leaderboard, my score is $0.23182$; I rank 2023 among the 2131 teams. In the private leaderboard, my score is $0.24015$; I rank 2025 among the 2131 teams.



\section{Problem Description}



\paragraph{Problem.}
The problem is to identify humpback whales on photos.
This is a binary classification and image classification problem in the same time.
The competition is online at \url{https://www.kaggle.com/c/humpback-whale-identification}.
This problem is very similar to face recognition problem explained in the course. We have photos of some whale's tails. Most of the photos are tagged by a given tail to a whale. Some of them are labeled as unknown. The assumption is the whale tails are unique like a person's face and we can recognize a whale by looking at its tail. 



\paragraph{Data.} 
The data are arbitrary JPEG images. Image sizes are not same neither their width, height, nor channels. Some of them are colorful, some are grayscale and sizes vary from 100 pixels to 2000 pixels or more.
The number of training samples is about $n=25,000$.
The total number of classes including unknown is $5005$.
The training set is not well-balanced. 

\paragraph{Challenges.}
There are lot of challenges here: First of all, We have imbalanced data set. the number of classes respect to instances are too much. We have $5005$ different classes in $25000$ instances. If even we consider it as balanced data set, we have average of $5$ instances per classes. In reality,  some classes just have one instance but some of them like unknowns have tens of them. 
Other challenge is that the amount of the classes; like face recognition problem, we need to classify photos as individual whales. This huge amount of classes make it difficult to use softmax as a classifier. Therefore I decided to use Siamese network \cite{chopra2005learning} to classify triplets.
Another challenge is the variety of inputs. Images are not in same size or even in same aspect ratio. Some are colorful and some are gray scale.

\section{Solution}

\paragraph{Model.}

The model I finally chose is the Open Face Recognition\cite{amos2016openface},  a deep convolutional neural network architecture, which is a variant of Inception model \cite{szegedy2015going}. It takes $192\times 192$ images and outputs a $128-dimensional$ feature vector. I used this network to compare and distinguish similar and non-similar pairs of images. To predict the class of test image we find nearest neighbors of the predicted feature vector in the space of training set.
A description of Keras Open Face is online: \url{http://krasserm.github.io/2018/02/07/deep-face-recognition/} and the open source code is also available online: \url{https://github.com/iwantooxxoox/Keras-OpenFace}.


\paragraph{Implementation.}
I implement the Open Face model using Keras with TensorFlow as the backend. 
My code is available at \url{https://github.com/saiedhp/CS583A-2019Spring/blob/master/project/humpback-whale-identification/kernel7631789da2.py}.
I run the code on a PC with one Intel i7 CPU and 64 GB memory.
It takes $20$ hours to train the model.
Model input is a triplet of images including anchor, positive and negative instances. The anchor is an image with a known class, positive is a different image in the same class as anchor, and negative is a different image in different class. I select the anchor image randomly from known classes (excluding unknown class), which have more than one member. Finally triplets of $(I_{anchor}, I_{positive}, I_{negative})$ passes to the model which a pair of  $(I_{anchor}, I_{positive})$ fed into the network and another pair of  $(I_{anchor}, I_{positive})$ fed into the same network with same weights. The similar pairs should be as much as possible close together while the non-similar pairs should be further in the feature space.
After training, I hoped the network can discriminate between the images by producing a feature vector. The most 5 nearest vector to the target vector have more chance to be a correct answer. I also tried to find a threshold to judge about the unknown class or unseen whales. The assumption is if I can not find a sample close enough to the target vector, it is better to classify it as an unknown.


\paragraph{Settings.}
The goal of the training is to increase the $L_2$ distance between non-similar classes and decrease the distance of similar classes in same time. The definition of the loss function is:
$$L=\sum_{i=1}^nmax [ 0, \|f(I_{anchor, i})-f(I_{positive, i})\|_2^2-\|f(I_{anchor, i})-f(I_{negative, i})\|_2^2+\alpha ] $$
 where $f(.)$ is the deep network which outputs the feature vector of input image, and $\alpha$ is the margin of this discrimination that I considered it as $1.0$. 
The loss function is a combination of $L_2$ distance and hinge loss.
The opitmizer is Adam.
I train the model with batch of $200$ for 50 epochs.


\paragraph{Advanced tricks.}
I used a pretrain model which was pretrained on human face. I expected it was a good point to start with high level feature extraction. It's true that human face is different to whale tails but it is also true that we need to extract basic feature to construct advanced features in middle and latest layers. 


\section{Compared Methods}


Before this model, I tried to classify the images based on a deep convolutional network, but it failed. The number of classes made it impossible to classify the images by softmax classifier. My final model is also awful but I don't have time to explore another methods and find better solution. Actually, this method is supposed to be good and I expected at least $80\%$ accuracy. It is possible that the inaccuracy comes from some bug in code development phase. I doubt if the implementation is wrong or model design.

\section{Outcome}


I participated in an inactive competition with late submission.
In the public leaderboard, my score is $0.23182$; we rank 2023 among the 2131 teams. In the private leaderboard, my score is $0.24015$; we rank 2025 among the 2131 teams.





%---------------------------------Figure---------------------------------%
%\begin{figure}
%	\begin{center}
%		\subfigure[Private leaderboard.]{\includegraphics[width=0.48\textwidth]{private.png}}
%		\subfigure[Public leaderboard.]{\includegraphics[width=0.48\textwidth]{public.png}}
%	\end{center}
%	\caption{Our rankings in the leaderboard.}
%	\label{fig:leaderboard}
%\end{figure}
%---------------------------------Figure---------------------------------%



%\vspace{3mm}
%\begin{lstlisting}
%import numpy
%
%def rfm(x, s, sigma):
%	n, d = x.shape
%	a = numpy.random.standard_normal((d, s)) / sigma
%	b = numpy.random.rand(1, s) * (2 * numpy.pi)
%	c = numpy.dot(x, a) + b
%	h = numpy.cos(c) * numpy.sqrt(2/s)
%	return h
%\end{lstlisting}
%\vspace{3mm}

%\newpage


\bibliographystyle{plain}
\bibliography{reference}


\end{document}